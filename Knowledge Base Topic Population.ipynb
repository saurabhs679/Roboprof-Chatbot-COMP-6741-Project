{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6286e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "162473c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Algorithm analysis.pdf to C:\\Users\\anubh\\Downloads\\Output\\Algorithm analysis.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Exception Handelling.pdf to C:\\Users\\anubh\\Downloads\\Output\\Exception Handelling.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Exception Handelling_Tutorial.pdf to C:\\Users\\anubh\\Downloads\\Output\\Exception Handelling_Tutorial.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Graphs.pdf to C:\\Users\\anubh\\Downloads\\Output\\Graphs.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Graphs_Tutorial.pdf to C:\\Users\\anubh\\Downloads\\Output\\Graphs_Tutorial.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Hashmap.pdf to C:\\Users\\anubh\\Downloads\\Output\\Hashmap.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Hashmap_Tutorial.pdf to C:\\Users\\anubh\\Downloads\\Output\\Hashmap_Tutorial.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Intelligent Agents.pdf to C:\\Users\\anubh\\Downloads\\Output\\Intelligent Agents.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Introduction.pdf to C:\\Users\\anubh\\Downloads\\Output\\Introduction.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Introduction_Tutorial.pdf to C:\\Users\\anubh\\Downloads\\Output\\Introduction_Tutorial.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Machine Learning for Intelligent Systems.pdf to C:\\Users\\anubh\\Downloads\\Output\\Machine Learning for Intelligent Systems.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Recursion.pdf to C:\\Users\\anubh\\Downloads\\Output\\Recursion.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Text Mining.pdf to C:\\Users\\anubh\\Downloads\\Output\\Text Mining.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Trees.pdf to C:\\Users\\anubh\\Downloads\\Output\\Trees.txt\n",
      "Converted C:\\Users\\anubh\\Downloads\\Slides_for_topics\\Trees_Tutorial.pdf to C:\\Users\\anubh\\Downloads\\Output\\Trees_Tutorial.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def save_text_to_file(text, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "def main(input_folder, output_folder):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        output_file = os.path.join(output_folder, os.path.splitext(filename)[0] + \".txt\")\n",
    "        \n",
    "        if filename.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(input_file)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            text = extract_text_from_docx(input_file)\n",
    "        else:\n",
    "            print(f\"Ignoring file: {filename}. Unsupported format.\")\n",
    "            continue\n",
    "        \n",
    "        # Perform any additional processing using spaCy if needed\n",
    "        # Example: doc = nlp(text)\n",
    "\n",
    "        save_text_to_file(text, output_file)\n",
    "        print(f\"Converted {input_file} to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"C:\\\\Users\\\\anubh\\\\Downloads\\\\Slides_for_topics\"\n",
    "  # Specify the directory containing your input PDFs or DOCX files\n",
    "    output_folder = \"C:\\\\Users\\\\anubh\\\\Downloads\\\\Output\"  # Specify the directory where you want to save the output text files\n",
    "    main(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82df0658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 79: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 46\u001b[0m     document_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Process the document with spaCy\u001b[39;00m\n\u001b[0;32m     49\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(document_text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 79: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the directory containing the processed text files\n",
    "directory = \"C:\\\\Users\\\\anubh\\\\Downloads\\\\Output\"\n",
    "\n",
    "# Define the API endpoint and headers\n",
    "url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "ai_ml_keywords = {\n",
    "    \"artificial\", \"intelligence\", \"ai\", \"machine\", \"learning\", \"ml\", \"intelligent\", \"system\",\n",
    "    \"programming\", \"algorithm\", \"deep\", \"neural\", \"network\", \"data\", \"analysis\", \"prediction\",\n",
    "    \"knowledge\", \"graphs\", \"text\", \"mining\", \"rdf\", \"content-based\", \"recommendations\",\n",
    "    \"collaborative\", \"filtering\", \"k-means\", \"clustering\", \"pattern-matching\", \"bots\",\n",
    "    \"natural\", \"language\", \"processing\", \"nlp\", \"inheritance\", \"tail\", \"recursion\",\n",
    "    \"efficiency\", \"hash\", \"table\", \"run\", \"time\", \"errors\", \"binary\", \"trees\", \"depth\",\n",
    "    \"first\", \"search\", \"dfs\"\n",
    "}\n",
    "\n",
    "# Function to filter named entities based on POS tags\n",
    "def filter_named_entities(doc):\n",
    "    named_entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"EVENT\", \"WORK_OF_ART\", \"PRODUCT\"]:\n",
    "            named_entities.append(ent.text)\n",
    "    return named_entities\n",
    "\n",
    "# Set to store seen URIs\n",
    "seen_uris = set()\n",
    "\n",
    "# Output file path\n",
    "output_file_path = \"output.txt\"\n",
    "\n",
    "# Open the output file in append mode\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Read the processed document from the file with specified encoding\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                document_text = file.read()\n",
    "\n",
    "            # Process the document with spaCy\n",
    "            doc = nlp(document_text)\n",
    "\n",
    "            # Filter named entities\n",
    "            named_entities = filter_named_entities(doc)\n",
    "\n",
    "            # Iterate over named entities and make requests to DBpedia Spotlight for each entity\n",
    "            for entity in named_entities:\n",
    "                # Make the GET request to DBpedia Spotlight\n",
    "                params = {\"text\": entity}\n",
    "                response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "                # Check if the request was successful\n",
    "                if response.status_code == 200:\n",
    "                    # Parse the JSON response\n",
    "                    data = response.json()\n",
    "                    # Write the annotations to the output file if not seen before\n",
    "                    if \"Resources\" in data:\n",
    "                        for resource in data[\"Resources\"]:\n",
    "                            if resource[\"@URI\"] not in seen_uris:\n",
    "                                # Print to console\n",
    "                                print(\"File:\", filename)\n",
    "                                print(\"Entity:\", entity)\n",
    "                                print(\"URI:\", resource[\"@URI\"])\n",
    "                                print(\"Surface Form:\", resource[\"@surfaceForm\"])\n",
    "                                print(\"Types:\", resource[\"@types\"])\n",
    "                                print()\n",
    "                                # Write to file\n",
    "                                output_file.write(\"File: \" + filename + \"\\n\")\n",
    "                                output_file.write(\"Entity: \" + entity + \"\\n\")\n",
    "                                output_file.write(\"URI: \" + resource[\"@URI\"] + \"\\n\")\n",
    "                                output_file.write(\"Surface Form: \" + resource[\"@surfaceForm\"] + \"\\n\")\n",
    "                                output_file.write(\"Types: \" + resource[\"@types\"] + \"\\n\\n\")\n",
    "                                seen_uris.add(resource[\"@URI\"])  # Add URI to seen URIs set\n",
    "                else:\n",
    "                    # Print error to console\n",
    "                    print(f\"Error for entity {entity} in file {filename}:\", response.status_code)\n",
    "                    # Write error to file\n",
    "                    output_file.write(f\"Error for entity {entity} in file {filename}: {response.status_code}\\n\")\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Output has been written to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a113ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define a function to format the URI\n",
    "def format_uri(entity):\n",
    "    return \"<http://dbpedia.org/resource/{}>\".format(entity.replace(\" \", \"_\"))\n",
    "\n",
    "# Define a function to format the TTL entry with additional properties\n",
    "def format_ttl(entity, surface_form, uri, course, lecture):\n",
    "    return \"\"\"\n",
    "    # Subject-Predicate-Object Triple\n",
    "    {} a ex:Topic ;\n",
    "        ex:TopicName \"{}\" ;\n",
    "        ex:dbpediaLink {} ;\n",
    "        ex:provenance \"Algorithm analysis\" ;\n",
    "        ex:topic_in_course {} ;\n",
    "        ex:topic_in_lecture {} .\n",
    "    \"\"\".format(uri, surface_form, uri, course, lecture)\n",
    "\n",
    "# Initialize an empty string to store the TTL content\n",
    "ttl_content = \"\"\n",
    "\n",
    "# Prefix Declarations\n",
    "prefixes = \"\"\"\n",
    "@prefix ex: <http://example.org/> .\n",
    "@prefix dbpedia: <http://dbpedia.org/resource/> .\n",
    "\"\"\"\n",
    "\n",
    "# Write the prefix declarations to the TTL content\n",
    "ttl_content += prefixes\n",
    "\n",
    "# Create a dictionary to store lecture-course mappings\n",
    "lecture_course_map = {}\n",
    "\n",
    "# Read the course info from the CSV file\n",
    "with open(\"Topic_information.csv\", newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        lecture_course_map[row['Lecture name']] = row['Course']\n",
    "\n",
    "# Open the input file and read line by line\n",
    "with open(\"C:\\\\Users\\\\anubh\\\\Downloads\\\\a.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize variables to store entity, URI, surface form, and lecture\n",
    "entity = \"\"\n",
    "uri = \"\"\n",
    "surface_form = \"\"\n",
    "lecture = \"\"\n",
    "\n",
    "# Iterate through the lines of the input file\n",
    "for line in lines:\n",
    "    # Remove leading/trailing whitespaces and check if the line is not empty\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        # Extract entity, URI, and surface form\n",
    "        if line.startswith(\"Entity:\"):\n",
    "            entity = line.split(\"Entity: \")[1]\n",
    "        elif line.startswith(\"URI:\"):\n",
    "            uri = line.split(\"URI: \")[1]\n",
    "        elif line.startswith(\"Surface Form:\"):\n",
    "            surface_form = line.split(\"Surface Form: \")[1]\n",
    "        elif line.startswith(\"File:\"):\n",
    "            lecture = line.split(\"File: \")[1].strip()[:-4]  # Remove the last 4 characters (\".txt\")\n",
    "            # Check if the lecture exists in the mapping\n",
    "            if lecture in lecture_course_map:\n",
    "                course = lecture_course_map[lecture]\n",
    "            else:\n",
    "                # Handle the case when the lecture is not found in the mapping\n",
    "                print(f\"Error: Lecture '{lecture}' not found in the course mapping.\")\n",
    "                continue\n",
    "        # Check if entity, URI, surface form, and lecture are all non-empty\n",
    "        if entity and uri and surface_form and lecture:\n",
    "            # Format and append TTL entry to the TTL content\n",
    "            ttl_content += format_ttl(entity, surface_form, format_uri(entity), course, lecture)\n",
    "            # Reset entity, URI, surface form, and lecture\n",
    "            entity = \"\"\n",
    "            uri = \"\"\n",
    "            surface_form = \"\"\n",
    "            lecture = \"\"\n",
    "\n",
    "# Write the TTL content to an output file\n",
    "with open(\"output.ttl\", \"w\") as file:\n",
    "    file.write(ttl_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "C:\\\\Users\\\\anubh\\\\Downloads\\\\a.txt\n",
    "# Define the API endpoint and parameters\n",
    "url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "params = {\n",
    "    \"text\": \"Concordia advanced six spots to 10th place among Canada’s engineering schools in the Maclean’s 2018 Program Rankings, while computer science advanced three spots into 11th position this year.\"\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    # Print the annotations\n",
    "    if \"Resources\" in data:\n",
    "        for resource in data[\"Resources\"]:\n",
    "            print(\"URI:\", resource[\"@URI\"])\n",
    "            print(\"Surface Form:\", resource[\"@surfaceForm\"])\n",
    "            print(\"Types:\", resource[\"@types\"])\n",
    "            print()\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b80843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
